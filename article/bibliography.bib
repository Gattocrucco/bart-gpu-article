@book{dlmf,
    date-modified = {2022-07-28 17:00:23 +0200},
    howpublished = {Release 1.1.6 of 2022-06-30},
    year = {2022},
    author = {F.~W.~J. Olver and A.~B. {Olde Daalhuis} and D.~W. Lozier and B.~I. Schneider and R.~F. Boisvert and C.~W. Clark and B.~R. Miller and B.~V. Saunders and H.~S. Cohl and M.~A. McClain and others},
    title = {NIST Digital Library of Mathematical Functions},
    url = {http://dlmf.nist.gov/},
    bdsk-url-1 = {http://dlmf.nist.gov/},
}

@book{rasmussen2006,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    date-added = {2021-07-14 17:27:09 +0200},
    date-modified = {2021-07-15 12:57:02 +0200},
    isbn = {0-262-18253-X},
    title = {Gaussian Processes for Machine Learning},
    url = {http://www.gaussianprocess.org/gpml/},
    year = 2006,
    Bdsk-Url-1 = {http://www.gaussianprocess.org/gpml/},
}

@book{stein1999,
    year = {1999},
    title = {Interpolation of Spatial Data},
    subtitle = {Some Theory for Kriging},
    author = {Stein, Michael L.},
    doi = {10.1007/978-1-4612-1494-6},
    series = {Springer Series in Statistics},
    publisher = {Springer New York, NY},
    issn = {0172-7397, 2197-568X},
    isbn = {978-0-387-98629-6, 978-1-4612-7166-6, 978-1-4612-1494-6},
}

@book{gramacy2020,
    author = {Gramacy, Robert B.},
    isbn = {978-0-367-\mbox{41542-6}},
    year = {2020},
    title = {Surrogates},
    subtitle = {Gaussian Process Modeling, Design, and Optimization for the Applied Sciences},
    publisher = {Chapman and Hall/CRC},
}

@book{wendland2004,
    place={Cambridge},
    series={Cambridge Monographs on Applied and Computational Mathematics},
    title={Scattered Data Approximation},
    DOI={10.1017/CBO9780511617539},
    publisher={Cambridge University Press},
    author={Wendland, Holger},
    year={2004},
    collection={Cambridge Monographs on Applied and Computational Mathematics},
    isbn = {978-0-511-61753-9, 978-0-521-13101-8}}

@book{gradshtein2014,
    title = {Table of Integrals, Series, and Products},
    author = {Gradshteyn, I. S. and Ryzhik, I. M.},
    editor = {Zwillinger, Daniel and Moll, Victor},
    url = {http://www.mathtable.com/gr},
    publisher = {Academic Press, Elsevier},
    isbn = {978-0-12-384933-5},
    year = {2014},
    edition = {8},
}

@misc{balog2016,
    doi = {10.48550/ARXIV.1606.05241},
    author = {Balog, Matej and Lakshminarayanan, Balaji and Ghahramani, Zoubin and Roy, Daniel M. and Teh, Yee Whye},
    keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {The Mondrian Kernel},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license},
    addendum = {Studies the correspondence of a
          kernel with forests, doing calculations similar to mine. Interesting
          for its bibliography of many related works.}
}

@article{chipman2010,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
    title = {{BART: Bayesian additive regression trees}},
    volume = {4},
    journal = {The Annals of Applied Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {266 -- 298},
    keywords = {Bayesian backfitting, boosting, CART, ‎classification‎, ensemble, MCMC, Nonparametric regression, probit model, random basis, regularizatio, sum-of-trees model, Variable selection, weak learner},
    year = {2010},
    doi = {10.1214/09-AOAS285},
    addendum = {Introduces BART.}
}

@inproceedings{he2019,
    title =      {XBART: Accelerated Bayesian Additive Regression Trees},
    author =       {He, Jingyu and Yalov, Saar and Hahn, P. Richard},
    booktitle =      {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
    pages =      {1130--1138},
    year =      {2019},
    editor =      {Chaudhuri, Kamalika and Sugiyama, Masashi},
    volume =      {89},
    series =      {Proceedings of Machine Learning Research},
    publisher =    {PMLR},
    url =      {https://proceedings.mlr.press/v89/he19a.html},
    abstract =      {Bayesian additive regression trees (BART) (Chipman et. al., 2010) is a powerful predictive model that often outperforms alternative models at out-of-sample prediction. BART is especially well-suited to settings with unstructured predictor variables and substantial sources of unmeasured variation as is typical in the social, behavioral and health sciences. This paper develops a modified version of BART that is amenable to fast posterior estimation. We present a stochastic hill climbing algorithm that matches the remarkable predictive accuracy of previous BART implementations, but is many times faster and less memory intensive.  Simulation studies show that the new method is comparable in computation time and more accurate at function estimation than both random forests and gradient boosting.},
    addendum = {Faster algorithm for BART inference.}
}

@article{hill2011,
    author = {Jennifer L. Hill},
    title = {Bayesian Nonparametric Modeling for Causal Inference},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {20},
    number = {1},
    pages = {217-240},
    year  = {2011},
    publisher = {Taylor & Francis},
    doi = {10.1198/jcgs.2010.08162},
    addendum = {Introduces the use of BART in causal inference.}
}


@InProceedings{rockova2019,
    title =      {On Theory for BART},
    author =       {Ro\v{c}kov\'a, Veronika and Saha, Enakshi},
    booktitle =      {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
    pages =      {2839--2848},
    year =      {2019},
    editor =      {Chaudhuri, Kamalika and Sugiyama, Masashi},
    volume =      {89},
    series =      {Proceedings of Machine Learning Research},
    month =      {4},
    publisher =    {PMLR},
    url =      {https://proceedings.mlr.press/v89/rockova19a.html},
    abstract =      {Ensemble learning is a statistical paradigm built on the premise that  many weak learners can perform exceptionally well when deployed collectively. The BART method of Chipman et al. (2010) is a prominent example of Bayesian ensemble learning, where each learner is a tree. Due to its impressive performance, BART has received a lot of attention from practitioners. Despite its wide popularity, however, theoretical studies of BART have  begun emerging only very recently. Laying down foundation for the theoretical analysis of Bayesian forests,  Rockova and van der Pas (2017) showed optimal posterior concentration under conditionally uniform tree priors. These priors  deviate from the actual priors implemented in BART. Here, we study the exact BART prior and propose a simple modification so that  it also enjoys optimality properties. To this end, we dive into the branching processes theory. We obtain  tail bounds for the distribution of total progeny under heterogeneous Galton-Watson (GW) processes using their connection to random walks. We conclude with a result stating  optimal rate of convergence for BART.},
    addendum = {Posterior concetration rates.}
}

@article{hill2020,
    title = {Bayesian Additive Regression Trees: A Review and Look Forward},
    DOI = {10.1146/annurev-statistics-031219-041110},
    abstractNote = {Bayesian additive regression trees (BART) provides a flexible approach to fitting a variety of regression models while avoiding strong parametric assumptions. The sum-of-trees model is embedded in a Bayesian inferential framework to support uncertainty quantification and provide a principled approach to regularization through prior specification. This article presents the basic approach and discusses further development of the original algorithm that supports a variety of data structures and assumptions. We describe augmentations of the prior specification to accommodate higher dimensional data and smoother functions. Recent theoretical developments provide justifications for the performance observed in simulations and other settings. Use of BART in causal inference provides an additional avenue for extensions and applications. We discuss software options as well as challenges and future directions.},
    journal = {Annual Review of Statistics and Its Application},
    volume = {7},
    number = {1},
    author = {Hill, Jennifer L. and Linero, Antonio R. and Murray, Jared S.},
    addendum = {A recent comprehensive review.},
    year = {2020}
}

@article{linero2018,
    author = {Linero, Antonio R.},
    title = {Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection},
    journal = {Journal of the American Statistical Association},
    volume = {113},
    number = {522},
    pages = {626-636},
    year  = {2018},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2016.1264957},
    addendum = {A modification of BART which is
            much better at variable selection.}
}

@misc{linero2022,
    doi = {10.48550/ARXIV.2210.16375},
    author = {Linero, Antonio R.},
    keywords = {Methodology (stat.ME), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SoftBart: Soft Bayesian Additive Regression Trees},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license},
    addendum = {The
          most recent, state of the art package for BART. Features priors on the
          hyperparameters and provides an interface to raw gibbs steps to use
          BART within other models (although not arbitrary ones).}
}

@misc{maia2022,
    doi = {10.48550/ARXIV.2204.02112},
    author = {Maia, Mateus and Murphy, Keefe and Parnell, Andrew C.},
    keywords = {Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
    addendum = {GP in the BART leaves.}
}

@misc{quiroga2022,
    doi = {10.48550/ARXIV.2206.03619},
    author = {Quiroga, Miriana and Garay, Pablo G and Alonso, Juan M. and Loyola, Juan Martin and Martin, Osvaldo A},
    keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Bayesian additive regression trees for probabilistic programming},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Share Alike 4.0 International},
    addendum = {An useful adaptation to use BART within PyMC. Note that
          they keep the hyperparameters fixed, and do not support more than one
          BART in the model.}
}

@misc{ronen2022,
    doi = {10.48550/ARXIV.2210.09352},
    author = {Ronen, Omer and Saarinen, Theo and Tan, Yan Shuo and Duncan, James and Yu, Bin},
    keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
    title = {A Mixing Time Lower Bound for a Simplified Version of BART},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license},
    addendum = {Says that BART's mcmc is slow to mix, with mixing time scaling
        badly with dataset size.}
}

@article{linero2017,
    doi = {10.29220/csam.2017.24.6.543},
    author = {Linero, Antonio R.},
    title = {A review of tree-based Bayesian methods},
    journal = {Communications for Statistical Applications and Methods},
    year = {2017},
    pages = {543–-559},
    volume = {24},
    number = {6},
}

@article{sparapani2021,
    title={Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART R Package},
    volume={97},
    doi={10.18637/jss.v097.i01},
    abstract={In this article, we introduce the BART R package which is an acronym for Bayesian additive regression trees. BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method for continuous, binary, categorical and time-to-event outcomes. Furthermore, BART is a tree-based, black-box method which fits the outcome to an arbitrary random function, f , of the covariates. The BART technique is relatively computationally efficient as compared to its competitors, but large sample sizes can be demanding. Therefore, the BART package includes efficient state-of-the-art implementations for continuous, binary, categorical and time-to-event outcomes that can take advantage of modern off-the-shelf hardware and software multi-threading technology. The BART package is written in C++ for both programmer and execution efficiency. The BART package takes advantage of multi-threading via forking as provided by the parallel package and OpenMP when available and supported by the platform. The ensemble of binary trees produced by a BART fit can be stored and re-used later via the R predict function. In addition to being an R package, the installed BART routines can be called directly from C++. The BART package provides the tools for your BART toolbox.},
    number={1},
    journal={Journal of Statistical Software},
    author={Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
    year={2021},
    pages={1–66}
}        

@article{hahn2020,
    author = {Hahn, P. Richard and Jared S. Murray and Carlos M. Carvalho},
    title = {{Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion)}},
    volume = {15},
    journal = {Bayesian Analysis},
    number = {3},
    publisher = {International Society for Bayesian Analysis},
    pages = {965 -- 2020},
    keywords = {Bayesian, Causal inference, heterogeneous treatment effects, machine learning, predictor-dependent priors, regression trees, regularization, shrinkage},
    year = {2020},
    doi = {10.1214/19-BA1195},
    addendum = {BCF.}
}

@misc{wang2022,
    doi = {10.48550/ARXIV.2204.10963},
    author = {Wang, Meijiang and He, Jingyu and Hahn, P. Richard},
    keywords = {Methodology (stat.ME), Econometrics (econ.EM), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Economics and business, FOS: Economics and business},
    title = {Local Gaussian process extrapolation for BART models with applications to causal inference},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
    addendum = {Graft a Gaussian process outside the grid to extrapolate better.}
}

@misc{li2022,
    doi = {10.48550/ARXIV.2206.15460},
    author = {Li, Fan and Ding, Peng and Mealli, Fabrizia},
    keywords = {Methodology (stat.ME), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Bayesian Causal Inference: A Critical Review},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
}

@article{dorie2019,
    author = {Vincent Dorie and Jennifer Hill and Uri Shalit and Marc Scott and Dan Cervone},
    title = {{Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition}},
    volume = {34},
    journal = {Statistical Science},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {43 -- 68},
    keywords = {automated algorithms, Causal inference, competition, evaluation, machine learning},
    year = {2019},
    doi = {10.1214/18-STS667},
}

@article{chipman1998,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
    title = {Bayesian CART Model Search},
    journal = {Journal of the American Statistical Association},
    volume = {93},
    number = {443},
    pages = {935-948},
    year  = {1998},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.1998.10473750},
    addendum = {The article which introduces the single-tree version of BART.}
}

@article{linero2018b,
    author = {Linero, Antonio R. and Yang, Yun},
    title = {Bayesian regression tree ensembles that adapt to smoothness and sparsity},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume = {80},
    number = {5},
    pages = {1087-1110},
    keywords = {Bayesian additive regression trees, Bayesian non-parametrics, High dimensional regimes, Model averaging, Posterior consistency},
    doi = {https://doi.org/10.1111/rssb.12293},
    url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12293},
    eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12293},
    abstract = {Summary Ensembles of decision trees are a useful tool for obtaining flexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classification and regression trees. Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing Bayesian additive regression tree algorithms.},
    year = {2018}
}

@book{tong1990,
    doi = {10.1007/978-1-4613-9655-0},
    year = {1990},
    publisher = {Springer New York},
    author = {Tong, Y. L.},
    title = {The Multivariate Normal Distribution},
    isbn = {978-1-4613-9657-4},
    edition = {1},
    issn = {0172-7397},
}

@article{scipy,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2},
}

@article{harris2020,
    title         = {Array programming with {NumPy}},
    author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
    year          = {2020},
    month         = {sep},
    journal       = {Nature},
    volume        = {585},
    number        = {7825},
    pages         = {357--362},
    doi           = {10.1038/s41586-020-2649-2},
    publisher     = {Springer Science and Business Media {LLC}},
    url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{pymc,
    doi = {10.7717/peerj-cs.55},
    url = {https://doi.org/10.7717/peerj-cs.55},
    year  = {2016},
    month = {apr},
    publisher = {{PeerJ}},
    volume = {2},
    pages = {e55},
    author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
    title = {Probabilistic programming in Python using {PyMC}3},
    journal = {{PeerJ} Computer Science}
}

@article{matplotlib,
    Author    = {Hunter, J. D.},
    Title     = {Matplotlib: A 2D graphics environment},
    Journal   = {Computing in Science \& Engineering},
    Volume    = {9},
    Number    = {3},
    Pages     = {90--95},
    abstract  = {Matplotlib is a 2D graphics package used for Python for
    application development, interactive scripting, and publication-quality
    image generation across user interfaces and operating systems.},
    publisher = {IEEE COMPUTER SOC},
    doi       = {10.1109/MCSE.2007.55},
    year      = 2007
}

@article{pratola2020,
author = {Pratola, Matthew T. and H. A. Chipman and E. I. George and R. E. McCulloch},
title = {Heteroscedastic BART via Multiplicative Regression Trees},
journal = {Journal of Computational and Graphical Statistics},
volume = {29},
number = {2},
pages = {405-417},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2019.1677243},

URL = { 
    
        https://doi.org/10.1080/10618600.2019.1677243
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10618600.2019.1677243
    
    

}

}

@article{kim2007,
author = { Hyunjoong   Kim  and  Wei-Yin   Loh  and  Yu-Shan   Shih  and  Probal   Chaudhuri },
title = {Visualizable and interpretable regression models with good prediction power},
journal = {IIE Transactions},
volume = {39},
number = {6},
pages = {565-579},
year  = {2007},
publisher = {Taylor & Francis},
doi = {10.1080/07408170600897502},

URL = { 
    
        https://doi.org/10.1080/07408170600897502
    
    

},
eprint = { 
    
        https://doi.org/10.1080/07408170600897502
    
    

}

}

@article{kapelner2016,
 title={bartMachine: Machine Learning with Bayesian Additive Regression Trees},
 volume={70},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v070i04},
 doi={10.18637/jss.v070.i04},
 abstract={We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.},
 number={4},
 journal={Journal of Statistical Software},
 author={Kapelner, Adam and Bleich, Justin},
 year={2016},
 pages={1–40}
}

@misc{jeong2022,
  doi = {10.48550/ARXIV.2008.06620},
  
  url = {https://arxiv.org/abs/2008.06620},
  
  author = {Jeong, Seonghyun and Rockova, Veronika},
  
  keywords = {Statistics Theory (math.ST), FOS: Mathematics, FOS: Mathematics},
  
  title = {The art of BART: Minimax optimality over nonhomogeneous smoothness in high dimension},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{imbens2015, place={Cambridge}, title={Causal Inference for
Statistics, Social, and Biomedical Sciences: An Introduction},
DOI={10.1017/CBO9781139025751}, publisher={Cambridge University Press},
author={Imbens, Guido W. and Rubin, Donald B.}, year={2015}}

@book{gelman2013, 
    author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
    year={2013},
    title={Bayesian Data Analysis},
    edition={3},
    publisher={Chapman and Hall/CRC},
    doi={10.1201/b16018},
    isbn={9780429113079},
    location={New York},
}

@inproceedings{gardner2018,
 author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
 url = {https://proceedings.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{quinonero2005,
  author  = {Joaquin Qui{{\~n}}onero-Candela and Carl Edward Rasmussen},
  title   = {A Unifying View of Sparse Approximate Gaussian Process Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {65},
  pages   = {1939--1959},
  url     = {http://jmlr.org/papers/v6/quinonero-candela05a.html}
}

@article{tan2019,
author = {Tan, Yaoyuan Vincent and Roy, Jason},
title = {Bayesian additive regression trees and the General BART model},
journal = {Statistics in Medicine},
volume = {38},
number = {25},
pages = {5048-5069},
keywords = {Bayesian nonparametrics, Dirichlet process mixtures, machine learning, semiparametric models, spatial},
doi = {https://doi.org/10.1002/sim.8347},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8347},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8347},
abstract = {Bayesian additive regression trees (BART) is a flexible prediction model/machine learning approach that has gained widespread popularity in recent years. As BART becomes more mainstream, there is an increased need for a paper that walks readers through the details of BART, from what it is to why it works. This tutorial is aimed at providing such a resource. In addition to explaining the different components of BART using simple examples, we also discuss a framework, the General BART model that unifies some of the recent BART extensions, including semiparametric models, correlated outcomes, and statistical matching problems in surveys, and models with weaker distributional assumptions. By showing how these models fit into a single framework, we hope to demonstrate a simple way of applying BART to research problems that go beyond the original independent continuous or binary outcomes framework.},
year = {2019}
}

@book{vandervaart1998,
    title = {Asymptotic Statistics},
    author = {{van der Vaart}, A. W.},
    year = {1998},
    publisher = {Cambridge University Press},
    isbn = {0 521 49603 9},
}

@misc{imai2022,
  doi = {10.48550/ARXIV.2203.14511},
  
  url = {https://arxiv.org/abs/2203.14511},
  
  author = {Imai, Kosuke and Li, Michael Lingzhi},
  
  keywords = {Methodology (stat.ME), Applications (stat.AP), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{imai2013,
author = {Kosuke Imai and Marc Ratkovic},
title = {{Estimating treatment effect heterogeneity in randomized program evaluation}},
volume = {7},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {443 -- 470},
keywords = {Causal inference, Individualized treatment rules, Lasso, moderation, Variable selection},
year = {2013},
doi = {10.1214/12-AOAS593},
URL = {https://doi.org/10.1214/12-AOAS593}
}

@article{linero2022b,
author = {Linero, Antonio R. and Antonelli, Joseph L.},
title = {The how and why of Bayesian nonparametric causal inference},
journal = {WIREs Computational Statistics},
year = {2022},
pages = {e1583},
keywords = {decision trees, Gaussian processes, mediation analysis, mixture models, nonparametrics},
doi = {https://doi.org/10.1002/wics.1583},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1583},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1583},
abstract = {Abstract Spurred on by recent successes in causal inference competitions, Bayesian nonparametric (and high-dimensional) methods have recently seen increased attention in the causal inference literature. In this article, we present a comprehensive overview of Bayesian nonparametric applications to causal inference. Our aims are to (i) introduce the fundamental Bayesian nonparametric toolkit; (ii) discuss how to determine which tool is most appropriate for a given problem; and (iii) show how to avoid common pitfalls in applying Bayesian nonparametric methods in high-dimensional settings. Unlike standard fixed-dimensional parametric problems, where outcome modeling alone can sometimes be effective, we argue that most of the time it is necessary to model both the selection and outcome processes. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Analysis of High Dimensional Data Statistical and Graphical Methods of Data Analysis > Nonparametric Methods Statistical and Graphical Methods of Data Analysis > Bayesian Methods and Theory}
}

@book{schott2017,
    author = {Schott, James R.},
    isbn = {9781119092483},
    title = {Matrix analysis for statistics},
    edition = {3},
    year = {2017},
    publisher = {John Wiley \& Sons},
    location = {Hoboken, New Jersey},
}

@misc{dua2019,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences",
}

@article{thal2023,
    author = {Thal, Dan R.C. and Finucane, Mariel M.},
    title = {Causal Methods Madness: Lessons Learned from the 2022 ACIC Competition to Estimate Health Policy Impacts},
    journal = {Observational Studies},
    volume = {9},
    number = {3},
    year = {2023},
    pages = {3-27},
    doi = {10.1353/obs.2023.0023},
}

@misc{deshpande2023,
      title={flexBART: Flexible Bayesian regression trees with categorical predictors}, 
      author={Sameer K. Deshpande},
      year={2023},
      eprint={2211.04459},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@inproceedings{gardner2018,
 author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{milios2018,
 author = {Milios, Dimitrios and Camoriano, Raffaello and Michiardi, Pietro and Rosasco, Lorenzo and Filippone, Maurizio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf},
 volume = {31},
 year = {2018}
}

@InProceedings{pleiss2018,
  title =    {Constant-Time Predictive Distributions for {G}aussian Processes},
  author =       {Pleiss, Geoff and Gardner, Jacob and Weinberger, Kilian and Wilson, Andrew Gordon},
  booktitle =    {Proceedings of the 35th International Conference on Machine Learning},
  pages =    {4114--4123},
  year =     {2018},
  editor =   {Dy, Jennifer and Krause, Andreas},
  volume =   {80},
  series =   {Proceedings of Machine Learning Research},
  month =    {10--15 Jul},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v80/pleiss18a/pleiss18a.pdf},
  url =      {https://proceedings.mlr.press/v80/pleiss18a.html},
  abstract =     {One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacrificing accuracy.}
}

@book{tsybakov2009,
  title={Introduction to Nonparametric Estimation},
  author={Tsybakov, Alexandre B.},
  series={Springer Series in Statistics},
  doi={10.1007/b13794},
  publisher={Springer New York, NY},
  date={2008-11-26},
  volumes={1},  
  pages={X, 214},
  isbn={978-0-387-79051-0},
  edition={1},
  topics={Statistical Theory and Methods, Probability and Statistics in Computer Science, Pattern Recognition, Econometrics, Signal, Image and Speech Processing, Probability Theory and Stochastic Processes},
  year={2009},
}

@book{vandervaart1998,
    place={Cambridge},
    series={Cambridge Series in Statistical and Probabilistic Mathematics},
    title={Asymptotic Statistics},
    DOI={10.1017/CBO9780511802256},
    publisher={Cambridge University Press},
    author={van der Vaart, A. W.},
    year={1998},
    collection={Cambridge Series in Statistical and Probabilistic Mathematics},
}

@book{ghosal2017,
    place={Cambridge}, 
    series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
    title={Fundamentals of Nonparametric Bayesian Inference}, 
    DOI={10.1017/9781139029834}, 
    publisher={Cambridge University Press}, 
    author={Ghosal, Subhashis and van der Vaart, Aad}, 
    year={2017}, 
    collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@misc{petrillo2022,
      title={The periodic zeta covariance function for Gaussian process regression}, 
      author={Giacomo Petrillo},
      year={2022},
      eprint={2208.02596},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@article {behr2020,
    author = {Merle Behr and Karl Kumbier and Aldo Cordova-Palomera and Matthew Aguire and Omer Ronen and Chengzhong Ye and Euan Ashley and Atul J. Butte and Rima Arnaout and Ben Brown and James Priest and Bin Yu},
    title = {Learning epistatic polygenic phenotypes with Boolean interactions},
    elocation-id = {2020.11.24.396846},
    year = {2023},
    doi = {10.1101/2020.11.24.396846},
    publisher = {Cold Spring Harbor Laboratory},
    abstract = {Detecting epistatic drivers of human phenotypes is a considerable challenge. Traditional approaches use regression to sequentially test multiplicative interaction terms involving pairs of genetic variants. For higher-order interactions and genome-wide large-scale data, this strategy is computationally intractable. Moreover, multiplicative terms used in regression modeling may not capture the form of biological interactions. Building on the Predictability, Computability, Stability (PCS) framework, we introduce the epiTree pipeline to extract higher-order interactions from genomic data using tree-based models. The epiTree pipeline first selects a set of variants derived from tissue-specific estimates of gene expression. Next, it uses iterative random forests (iRF) to search training data for candidate Boolean interactions (pairwise and higher-order). We derive significance tests for interactions, based on a stabilized likelihood ratio test, by simulating Boolean tree-structured null (no epistasis) and alternative (epistasis) distributions on hold-out test data. Finally, our pipeline computes PCS epistasis p-values that probabilisticly quantify improvement in prediction accuracy via bootstrap sampling on the test set. We validate the epiTree pipeline in two case studies using data from the UK Biobank: predicting red hair and multiple sclerosis (MS). In the case of predicting red hair, epiTree recovers known epistatic interactions surrounding MC1R and novel interactions, representing non-linearities not captured by logistic regression models. In the case of predicting MS, a more complex phenotype than red hair, epiTree rankings prioritize novel interactions surrounding HLA-DRB1, a variant previously associated with MS in several populations. Taken together, these results highlight the potential for epiTree rankings to help reduce the design space for follow up experiments.Competing Interest StatementThe authors have declared no competing interest.},
    URL = {https://www.biorxiv.org/content/early/2023/11/22/2020.11.24.396846},
    eprint = {https://www.biorxiv.org/content/early/2023/11/22/2020.11.24.396846.full.pdf},
    journal = {bioRxiv}
}

@article{huang2017,
    abstract = {All fields of science are now inundated with massive amounts of data, which have the potential to answer fundamental questions. Genomics is one particular example, exploring questions like: How does the human genome work? What genome variants make us more prone to diseases? To find answers to these questions, it is crucial to develop statistical and machine learning methods that can scale up, particularly through efficient data storage and communication. Equally crucial, but less emphasized, is the possession of data wisdom---a rebranding of the best elements of applied statistics in a recent note at ODBMS.org (http://www.odbms.org/2015/04/data-wisdom-for-data-science/). The note at ODBMS.org contains ten sets of questions a practitioner can ask to cultivate data wisdom. Although there has been much recent excitement about big data, having enough data relevant to the problem is the key to gaining meaningful answers in genomics. Data wisdom gives us the insight into how these data would look, how much information a dataset really contains, and how to extract it. In this paper, we expand on the ten sets of questions and illustrate where and how data wisdom can be integrated into computational genomics research.},
    author = {Huang, Haiyan and Yu, Bin},
    date = {2017/12/01},
    date-added = {2023-12-05 20:31:16 -0600},
    date-modified = {2023-12-05 20:31:16 -0600},
    doi = {10.1007/s12561-016-9173-9},
    id = {Huang2017},
    isbn = {1867-1772},
    journal = {Statistics in Biosciences},
    number = {2},
    pages = {646--661},
    title = {Data Wisdom in Computational Genomics Research},
    url = {https://doi.org/10.1007/s12561-016-9173-9},
    volume = {9},
    year = {2017},
    bdsk-url-1 = {https://doi.org/10.1007/s12561-016-9173-9},
}

@article{kennedy2023,
    author = {Edward H. Kennedy},
    title = {{Towards optimal doubly robust estimation of heterogeneous causal effects}},
    volume = {17},
    journal = {Electronic Journal of Statistics},
    number = {2},
    publisher = {Institute of Mathematical Statistics and Bernoulli Society},
    pages = {3008 -- 3049},
    keywords = {Conditional effects, influence function, Minimax rate, Nonparametric regression},
    year = {2023},
    doi = {10.1214/23-EJS2157},
    URL = {https://doi.org/10.1214/23-EJS2157}
}

@article{little2004,
    ISSN = {10170405, 19968507},
    URL = {http://www.jstor.org/stable/24307424},
    abstract = {The model-based approach to inference from multivariate data with missing values is reviewed. Regression prediction is most useful when the covariates are predictive of the missing values and the probability of being missing, and in these circumstances predictions are particularly sensitive to model misspecification. The use of penalized splines of the propensity score is proposed to yield robust model-based inference under the missing at random (MAR) assumption, assuming monotone missing data. Simulation comparisons with other methods suggest that the method works well in a wide range of populations, with little loss of efficiency relative to parametric models when the latter are correct. Extensions to more general patterns are outlined.},
    author = {Roderick Little and Hyonggin An},
    journal = {Statistica Sinica},
    number = {3},
    pages = {949--968},
    publisher = {Institute of Statistical Science, Academia Sinica},
    title = {Robust Likelihood-Based Analysis of Multivariate Data with Missing Values},
    urldate = {2024-01-31},
    volume = {14},
    year = {2004}
}

@article{kang2007,
    ISSN = {08834237},
    URL = {http://www.jstor.org/stable/27645858},
    abstract = {When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
    author = {Joseph D. Y. Kang and Joseph L. Schafer},
    journal = {Statistical Science},
    number = {4},
    pages = {523--539},
    publisher = {Institute of Mathematical Statistics},
    title = {Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data},
    urldate = {2024-01-31},
    volume = {22},
    year = {2007}
}

@article{zhang2009,
    author = {Zhang, Guangyu and Little, Roderick},
    title = {Extensions of the Penalized Spline of Propensity Prediction Method of Imputation},
    journal = {Biometrics},
    volume = {65},
    number = {3},
    pages = {911-918},
    keywords = {Missing at random, Penalized spline, Propensity},
    doi = {https://doi.org/10.1111/j.1541-0420.2008.01155.x},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2008.01155.x},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2008.01155.x},
    abstract = {Summary Little and An (2004, Statistica Sinica 14, 949–968) proposed a penalized spline of propensity prediction (PSPP) method of imputation of missing values that yields robust model-based inference under the missing at random assumption. The propensity score for a missing variable is estimated and a regression model is fitted that includes the spline of the estimated logit propensity score as a covariate. The predicted unconditional mean of the missing variable has a double robustness (DR) property under misspecification of the imputation model. We show that a simplified version of PSPP, which does not center other regressors prior to including them in the prediction model, also has the DR property. We also propose two extensions of PSPP, namely, stratified PSPP and bivariate PSPP, that extend the DR property to inferences about conditional means. These extended PSPP methods are compared with the PSPP method and simple alternatives in a simulation study and applied to an online weight loss study conducted by Kaiser Permanente.},
    year = {2009}
}

@article{tsiatis2007,
    address = {Anastasios A. Tsiatis is Drexel Professor of Statistics at North Carolina State University, Raleigh, North Carolina 27695-8203, USA (e-mail: tsiatis{@}stat.ncsu.edu ).},
    author = {Tsiatis, Anastasios A and Davidian, Marie},
    crdt = {2008/06/03 09:00},
    date = {2007},
    doi = {10.1214/07-STS227},
    edat = {2008/06/03 09:00},
    gr = {R01 CA051962-17A1/CA/NCI NIH HHS/United States; R01 CA085848-07/CA/NCI NIH HHS/United States; R01 CA051962-16/CA/NCI NIH HHS/United States; P50 DA010075/DA/NIDA NIH HHS/United States; P50 DA010075-100010/DA/NIDA NIH HHS/United States; R37 AI031789-17/AI/NIAID NIH HHS/United States; R01 CA085848-08A1/CA/NCI NIH HHS/United States; R37 AI031789/AI/NIAID NIH HHS/United States; R37 AI031789-18/AI/NIAID NIH HHS/United States; R01 CA085848/CA/NCI NIH HHS/United States; R01 CA051962/CA/NCI NIH HHS/United States},
    issn = {0883-4237 (Print); 0883-4237 (Linking)},
    jid = {100962994},
    journal = {Stat Sci},
    jt = {Statistical science : a review journal of the Institute of Mathematical Statistics},
    language = {eng},
    lr = {20220317},
    mhda = {2008/06/03 09:01},
    mid = {NIHMS48048},
    number = {4},
    own = {NLM},
    pages = {569--573},
    phst = {2008/06/03 09:00 {$[$}pubmed{$]$}; 2008/06/03 09:01 {$[$}medline{$]$}; 2008/06/03 09:00 {$[$}entrez{$]$}},
    pl = {United States},
    pmc = {PMC2397555},
    pmid = {18516239},
    pst = {ppublish},
    pt = {Journal Article},
    status = {PubMed-not-MEDLINE},
    title = {Comment: Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.},
    volume = {22},
    year = {2007},
}

@article{papadogeorgou2019,
    author = {Georgia Papadogeorgou and Fan Li},
    title = {Discussion of “Penalized Spline of Propensity Methods for Treatment Comparison”},
    journal = {Journal of the American Statistical Association},
    volume = {114},
    number = {525},
    pages = {32-35},
    year = {2019},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2018.1543120},
}


@article{chen2019,
    author = {Qingxia Chen and Frank E. Harrell Jr.},
    title = {Comment: Penalized Spline of Propensity Methods for Treatment Comparison},
    journal = {Journal of the American Statistical Association},
    volume = {114},
    number = {525},
    pages = {28-30},
    year = {2019},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2018.1537915},
}

@article{spieker2019,
    author = {Andrew J. Spieker},
    title = {Comment on Penalized Spline of Propensity Methods for Treatment Comparison by Zhou, Elliott, and Little},
    journal = {Journal of the American Statistical Association},
    volume = {114},
    number = {525},
    pages = {20-23},
    year = {2019},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2018.1537913},
}

@article{zhou2019b,
    author = {Tingting Zhou, Michael R. Elliott and Roderick J. A. Little},
    title = {Penalized Spline of Propensity Methods for Treatment Comparison: Rejoinder},
    journal = {Journal of the American Statistical Association},
    volume = {114},
    number = {525},
    pages = {35-38},
    year = {2019},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2019.1576439},
}

@article{zhou2019,
    author = {Tingting Zhou, Michael R. Elliott and Roderick J. A. Little},
    title = {Penalized Spline of Propensity Methods for Treatment Comparison},
    journal = {Journal of the American Statistical Association},
    volume = {114},
    number = {525},
    pages = {1-19},
    year = {2019},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2018.1518234},
}

@misc{graumoya2024,
    title={Learning Universal Predictors}, 
    author={Jordi Grau-Moya and Tim Genewein and Marcus Hutter and Laurent Orseau and Grégoire Delétang and Elliot Catt and Anian Ruoss and Li Kevin Wenliang and Christopher Mattern and Matthew Aitchison and Joel Veness},
    year={2024},
    eprint={2401.14953},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{pratola2016,
    author = {Matthew T. Pratola},
    title = {{Efficient Metropolis–Hastings Proposal Mechanisms for Bayesian Regression Tree Models}},
    volume = {11},
    journal = {Bayesian Analysis},
    number = {3},
    publisher = {International Society for Bayesian Analysis},
    pages = {885 -- 911},
    keywords = {computer experiments, coverage probability, credible interval, Markov chain Monte Carlo, proposal distribution, uncertainty quantification},
    year = {2016},
    doi = {10.1214/16-BA999},
    URL = {https://doi.org/10.1214/16-BA999}
}

@Inbook{solomonoff2009,
    author="Solomonoff, Ray J.",
    editor="Emmert-Streib, Frank and Dehmer, Matthias",
    title="Algorithmic Probability: Theory and Applications",
    bookTitle="Information Theory and Statistical Learning",
    year="2009",
    publisher="Springer US",
    address="Boston, MA",
    pages="1--23",
    abstract="We first define Algorithmic Probability, an extremely powerful method of inductive inference. We discuss its completeness, incomputability, diversity and subjectivity and show that its incomputability in no way inhibits its use for practical prediction. Applications to Bernoulli sequence prediction and grammar discovery are described. We conclude with a note on its employment in a very strong AI system for very general problem solving.",
    isbn="978-0-387-84816-7",
    doi="10.1007/978-0-387-84816-7_1",
    url="https://doi.org/10.1007/978-0-387-84816-7_1"
}

@article{johnstone2005a,
    author = {Iain M. Johnstone and Bernard W. Silverman},
    title = {{Empirical Bayes selection of wavelet thresholds}},
    volume = {33},
    journal = {The Annals of Statistics},
    number = {4},
    publisher = {Institute of Mathematical Statistics},
    pages = {1700 -- 1752},
    keywords = {Adaptivity, Bayesian inference, Nonparametric regression, smoothing, Sparsity},
    year = {2005},
    doi = {10.1214/009053605000000345},
    URL = {https://doi.org/10.1214/009053605000000345}
}

@article{johnstone2005b,
    title={EbayesThresh: R Programs for Empirical Bayes Thresholding},
    volume={12},
    url={https://www.jstatsoft.org/index.php/jss/article/view/v012i08},
    doi={10.18637/jss.v012.i08},
    abstract={Suppose that a sequence of unknown parameters is observed sub ject to independent Gaussian noise. The EbayesThresh package in the S language implements a class of Empirical Bayes thresholding methods that can take advantage of possible sparsity in the sequence, to improve the quality of estimation. The prior for each parameter in the sequence is a mixture of an atom of probability at zero and a heavy-tailed density. Within the package, this can be either a Laplace (double exponential) density or else a mixture of normal distributions with tail behavior similar to the Cauchy distribution. The mixing weight, or sparsity parameter, is chosen automatically by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold, and the package provides the posterior mean, and hard and soft thresholding, as additional options. This paper reviews the method, and gives details (far beyond those previously published) of the calculations needed for implementing the procedures. It explains and motivates both the general methodology, and the use of the EbayesThresh package, through simulated and real data examples. When estimating the wavelet transform of an unknown function, it is appropriate to apply the method level by level to the transform of the observed data. The package can carry out these calculations for wavelet transforms obtained using various packages in R and S-PLUS. Details, including a motivating example, are presented, and the application of the method to image estimation is also explored. The final topic considered is the estimation of a single sequence that may become progressively sparser along the sequence. An iterated least squares isotone regression method allows for the choice of a threshold that depends monotonically on the order in which the observations are made. An alternative possibility, also discussed in detail, is a particular parametric dependence of the sparsity parameter on the position in the sequence.},
    number={8},
    journal={Journal of Statistical Software},
    author={Johnstone, Iain and Silverman, Bernard W.},
    year={2005},
    pages={1–38}
}

@article{horii2023,
    title={Uncertainty Quantification in Heterogeneous Treatment Effect Estimation with Gaussian-Process-Based Partially Linear Model},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/30025},
    DOI={10.1609/aaai.v38i18.30025},
    abstractNote={Estimating heterogeneous treatment effects across individuals has attracted growing attention as a statistical tool for performing critical decision-making. We propose a Bayesian inference framework that quantifies the uncertainty in treatment effect estimation to support decision-making in a relatively small sample size setting. Our proposed model places Gaussian process priors on the nonparametric components of a semiparametric model called a partially linear model. This model formulation has three advantages. First, we can analytically compute the posterior distribution of a treatment effect without relying on the computationally demanding posterior approximation. Second, we can guarantee that the posterior distribution concentrates around the true one as the sample size goes to infinity. Third, we can incorporate prior knowledge about a treatment effect into the prior distribution, improving the estimation efficiency. Our experimental results show that even in the small sample size setting, our method can accurately estimate the heterogeneous treatment effects and effectively quantify its estimation uncertainty.},
    number={18},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Horii, Shunsuke and Chikahara, Yoichi},
    year={2024},
    month={Mar.},
    pages={20420-20429}
}

@article{kinderman1977,
    author = {Kinderman, A. J. and Monahan, J. F.},
    title = {Computer Generation of Random Variables Using the Ratio of Uniform Deviates},
    year = {1977},
    issue_date = {Sept. 1977},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {3},
    issn = {0098-3500},
    url = {https://doi.org/10.1145/355744.355750},
    doi = {10.1145/355744.355750},
    journal = {ACM Trans. Math. Softw.},
    month = {sep},
    pages = {257–260},
    numpages = {4}
}

@misc{hahn2019,
    title={Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017}, 
    author={Hahn, P. Richard and Vincent Dorie and Jared S. Murray},
    year={2019},
    eprint={1905.09515},
    archivePrefix={arXiv},
    primaryClass={stat.ME},
    url={https://arxiv.org/abs/1905.09515}, 
}

@online{acic2019,
  title = {Atlantic Causal Inference Conference 2019 Data Challenge},
  author = {Gruber, Susan and Lefebvre, Geneviève and Schuster, Tibor and Piché, Alexandre},
  url = {https://sites.google.com/view/ACIC2019DataChallenge},
  urldate = {2024-09-02},
  year = {2019},
  organization = {Atlantic Causal Inference Conference}
}

@inproceedings{arora2019,
    author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {On Exact Computation with an Infinitely Wide Neural Net},
    url = {https://proceedings.neurips.cc/paper/2019/hash/dbc4d84bfcfe2284ba11beffb853a8c4-Abstract.html},
    volume = {32},
    year = {2019}
}

@inproceedings{chen2016,
    author = {Chen, Tianqi and Guestrin, Carlos},
    title = {XGBoost: A Scalable Tree Boosting System},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2939785},
    doi = {10.1145/2939672.2939785},
    abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {785–794},
    numpages = {10},
    keywords = {large-scale machine learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}

@online{kaggle2021,
    author = {{Kaggle}},
    title = {State of Data Science and Machine Learning 2021},
    year = {2021},
    url = {https://www.kaggle.com/kaggle-survey-2021},
    urldate = {2024-09-19},
    organization = {Kaggle},
}

@article{he2021,
    author = {He, Jingyu and Hahn, P. Richard},
    title = {Stochastic Tree Ensembles for Regularized Nonlinear Regression},
    journal = {Journal of the American Statistical Association},
    volume = {118},
    number = {541},
    pages = {551--570},
    year = {2021},
    publisher = {Journal of the American Statistical Association},
    doi = {10.1080/01621459.2021.1942012},
}

@software{bradbury2018,
    author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
    title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
    url = {http://github.com/jax-ml/jax},
    version = {0.4.33},
    year = {2018},
}

@article{pratola2014,
     ISSN = {10618600},
     URL = {http://www.jstor.org/stable/43304924},
     abstract = {Bayesian additive regression trees (BART) is a Bayesian approach to flexible nonlinear regression which has been shown to be competitive with the best modern predictive methods such as those based on bagging and boosting. BART offers some advantages. For example, the stochastic search Markov chain Monte Carlo (MCMC) algorithm can provide a more complete search of the model space and variation across MCMC draws can capture the level of uncertainty in the usual Bayesian way. The BART prior is robust in that reasonable results are typically obtained with a default prior specification. However, the publicly available implementation of the BART algorithm in the R package Bayes Tree is not fast enough to be considered interactive with over a thousand observations, and is unlikely to even run with 50,000 to 100,000 observations. In this article we show how the BART algorithm may be modified and then computed using single program, multiple data (SPMD) parallel computation implemented using the Message Passing Interface (MPI) library. The approach scales nearly linearly in the number of processor cores, enabling the practitioner to perform statistical inference on massive datasets. Our approach can also handle datasets too massive to fit on any single data repository.},
     author = {Matthew T. Pratola and Hugh A. Chipman and James R. Gattiker and David M. Higdon and Robert McCulloch and William N. Rust},
     journal = {Journal of Computational and Graphical Statistics},
     number = {3},
     pages = {830--852},
     publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
     title = {Parallel Bayesian Additive Regression Trees},
     urldate = {2024-09-25},
     volume = {23},
     year = {2014}
}

@inproceedings{chipman2006,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {265--272},
 publisher = {MIT Press},
 title = {Bayesian Ensemble Learning},
 url = {https://proceedings.neurips.cc/paper/2006/hash/1706f191d760c78dfcec5012e43b6714-Abstract.html},
 volume = {19},
 year = {2006}
}

@book{daniels2023,
  author    = {Daniels, Michael J. and Linero, Antonio R. and Roy, Jason},
  title     = {Bayesian Nonparametrics for Causal Inference and Missing Data},
  year      = {2023},
  edition   = {1},
  publisher = {Chapman and Hall/CRC},
  doi       = {10.1201/9780429324222}
}

@software{petrillo2024b,
  author       = {Giacomo Petrillo},
  title        = {{Gattocrucco/bartz: The real treasure was the 
                   Markov chain samples we made along the way}},
  month        = oct,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.13931478},
  url          = {https://doi.org/10.5281/zenodo.13931478}
}

@software{dorie2024,
    author = {Dorie, Vincent and
          Chipman, Hugh and
          McCulloch, Robert and
          Dadgar, Armon and
          {R Core Team} and
          Draheim, Guido U. and
          Bosmans, Maarten and
          Tournayre, Christophe and
          Petch, Michael and
          Valle, Rafael de Lucena and
          Johnson, Steven G. and
          Frigo, Matteo and
          Zaitseff, John and
          Veldhuizen, Todd and
          Maisonobe, Luc and
          Pakin, Scott and
          Richard G., Daniel},
    title = {dbarts: Discrete Bayesian Additive Regression Trees Sampler},
    year = {2024},
    version = {0.9-28},
    url = {https://CRAN.R-project.org/package=dbarts},
}

@software{kapelner2023,
    author = {Kapelner, Adam and Bleich, Justin},
    title = {bartMachine: Bayesian Additive Regression Trees},
    year = {2023},
    version = {1.3.4.1},
    url = {https://cran.r-project.org/package=bartMachine},
}

@software{mcculloch2024,
    author = {McCulloch, Robert and Sparapani, Rodney and Gramacy, Robert and Pratola, Matthew  and Spanbauer, Charles  and Plummer, Martyn  and Best, Nicky  and Cowles, Kate  and Vines, Karen },
    title = {BART: Bayesian Additive Regression Trees},
    year = {2024},
    version = {2.9.9},
    url = {https://cran.r-project.org/package=BART},
}

@software{chipman2024,
    author = {Chipman, Hugh A. and McCulloch, Robert },
    title = {BayesTree: Bayesian Additive Regression Trees},
    year = {2024},
    version = {0.3-1.5},
    url = {https://cran.r-project.org/package=BayesTree},
}

@book{muller2015,
  title = {Bayesian Nonparametric Data Analysis},
  author = {M{\"u}ller, Peter and Quintana, Fernando Andres and Jara, Alejandro and Hanson, Tim},
  year = {2015},
  publisher = {Springer Cham},
  series = {Springer Series in Statistics},
  doi = {10.1007/978-3-319-18968-0},
  isbn = {978-3-319-18967-3},
  edition = {1},
  pages = {XIV, 193},
}

@book{brooks2011,
  title={Handbook of Markov Chain Monte Carlo},
  editor={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={Chapman and Hall/CRC},
  address={New York},
  edition={1},
  isbn={9780429138508},
  doi={10.1201/b10905},
  pages={619},
  note={eBook published on 24 May 2011}
}

@misc{petrillo2024f,
      title={On the Gaussian process limit of Bayesian Additive Regression Trees}, 
      author={Giacomo Petrillo},
      year={2024},
      eprint={2410.20289},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2410.20289}, 
}

@software{petrillo2024e,
  author       = {Giacomo Petrillo},
  title        = {Gattocrucco/bart-gpu-article: First version},
  month        = oct,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {first-version},
  doi          = {10.5281/zenodo.14010505},
  url          = {https://doi.org/10.5281/zenodo.14010505}
}

@misc{tan2024,
      title={The Computational Curse of Big Data for Bayesian Additive Regression Trees: A Hitting Time Analysis}, 
      author={Yan Shuo Tan and Omer Ronen and Theo Saarinen and Bin Yu},
      year={2024},
      eprint={2406.19958},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.19958}, 
}
