\documentclass{article}

% \usepackage{icml2025} % Use the following line for the initial blind version submitted for review
\usepackage[accepted]{icml2025} % If accepted, instead use the following line for the camera-ready submission

% Attempt to make hyperref and algorithmic work together better:
\renewcommand{\theHalgorithm}{\arabic{algorithm}}

% math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}

% code
\usepackage{listings}

% lists
\usepackage{enumitem}

% links and pdf index
\usepackage{hyperref}
\renewcommand{\subsectionautorefname}{section}
\newcommand{\algorithmautorefname}{Algorithm}

% images
\usepackage{graphicx}
% command to center the contents even if they are larger than the text width
\newcommand{\widecenter}[1]{\noindent\hspace{-\textwidth}\makebox[3\textwidth][c]{#1}}
\newcommand{\halfcenter}[1]{\noindent\hspace{-\columnwidth}\makebox[3\columnwidth][c]{#1}}
% command to include a pdf generated by matplotlib, 0.703125 = 360pt / 6.4in * 9/10
\newcommand{\includempl}[1]{\includegraphics[scale=0.703125]{#1}}

% smaller margin notes
\let\oldmarginpar\marginpar
\renewcommand{\marginpar}[1]{\oldmarginpar{\sffamily\scriptsize #1}}
\setlength{\marginparwidth}{3.5cm}
\renewcommand{\marginpar}[1]{\relax} % hide marginpars

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}


\begin{document}

\twocolumn[
\icmltitle{Very Fast Bayesian Additive Regression Trees on GPU}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
    \icmlauthor{Giacomo Petrillo}{unifi}
\end{icmlauthorlist}

\icmlaffiliation{unifi}{Department of Statistics, Computer Science,
Applications ``Giuseppe Parenti'' (DISIA), University of Florence, Italy}

\icmlcorrespondingauthor{Giacomo Petrillo}{giacomo.petrillo@unifi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


    \begin{abstract}
        
        Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian regression technique based on an ensemble of decision trees. It is part of the toolbox of many statisticians. The overall statistical quality of the regression is typically higher than other generic alternatives, and it requires less manual tuning, making it a good default choice. However, it is a niche method compared to its natural competitor \texttt{XGBoost}, due to the longer running time, making sample sizes above \num{10000}--\num{100000} a nuisance. I present a GPU-enabled implementation of BART, faster by up to 200x relative to a single CPU core, making BART competitive in running time with \texttt{XGBoost}. This implementation is available in the Python package \texttt{bartz}.

    \end{abstract}

    \section{Introduction}
    \label{sec:intro}

    \paragraph{BART}

    Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian regression method, introduced by \citet{chipman2006,chipman2010}. It defines a prior distribution over the space of functions by representing them as a sum of binary decision trees, and then specifying a stochastic tree generation process. The posterior is then obtained with Metropolis-Gibbs sampling over the trees. See \citet{hill2020} for a review, and \citet[ch.~5]{daniels2023} for a textbook treatment.

    \paragraph{BART's success}
    
    BART has proven empirically effective, and is gaining popularity \citep[consider, e.g.,][]{tan2019}. The Atlantic Causal Inference Conference (ACIC) Data Challenge has confirmed BART as one of the best regression methods for causal inference \citep{dorie2019,hahn2019,acic2019,thal2023}. Many BART variants have been developed throughout the years, adding features such as variable selection \citep{linero2018}.

    \paragraph{BART vs.\ XGBoost}

    According to the \citet[p.~35]{kaggle2021} survey, the most popular regression method in the class of BART is the variant of gradient boosting implemented in the software \texttt{XGBoost} \citep{chen2016}, used by half of the surveyed data scientists. \citet[table~2, p.~1105]{linero2018b} show that BART predicts better than \texttt{XGBoost} on average on a set of benchmark datasets (with both methods cross validated). The other advantages of BART over \texttt{XGBoost} are that, due to being Bayesian, it provides full uncertainty quantification out of the box, a principled way to extend the model for special settings, and good results even without cross validation. However, BART is much slower than \texttt{XGBoost}, making it impractical for large datasets: the largest BART analysis I know of had $n\approx\num{500000}$ observations/examples and $p=90$ predictors/features \citep{pratola2020}, while \texttt{XGBoost} boasts ``billions of examples'' on its home page.

    \paragraph{XBART}

    \citet{he2019} and \citet{he2021} introduced Accelerated BART (XBART), a BART variant that partially sacrifices Bayesianism to run 20--30 times faster. They show XBART is overall better at prediction than Random Forests and \texttt{XGBoost}, by working well in both high and low noise settings. However, to this day, XBART seems to not have gained traction. I believe this is due to the lack of a welcoming software ecosystem and publicity, rather than a problem with the method. XBART, not being fully Bayesian, also loses the advantage of a well-defined way of extending the model, which is important in Statistics.

    \paragraph{BART on GPU}

    To tackle this challenge, and bring the features of BART to large datasets, I re-implemented the original BART algorithm for GPU. My implementation yields a speed gain of up to 200 times, comparing a Nvidia A100 GPU against a single Apple M1 Pro CPU core. The maximum gain can be reached only with sample size $n \gtrsim \num{1000000}$ and  number of predictors and number of trees $p,n_\text{tree}\lesssim\num{10000}$, but these limits look solvable with additional work. The memory usage is $n(p+n_\text{tree})$ bytes, about 20 times less than the previous fastest implementation. I provide a Python package,
    \texttt{bartz}
    \citep{petrillo2024b}%
    , which mimics the most common BART interface found in R packages. In \autoref{sec:bart} I summarize the background knowledge on BART, then in \autoref{sec:impl} I describe how to adapt BART to run on GPU, and in \autoref{sec:perf} I measure the performance of my implementation. Finally, in \autoref{sec:conclusions} I assess the merits of my work and consider further research directions.

    \section{BART recap}
    \label{sec:bart}

    \paragraph{Bayesian nonparametric regression}

    BART is a Bayesian nonparametric regression method. Semi-formally, this means that, given a set of $n$ pairs of observations $(\mathbf x_i,y_i) \in \mathbb R^p \times \mathbb R$, it infers without guidance a function $f$ such that $y_i = f(\mathbf x_i) + \varepsilon_i$, keeping a balance between $f$ and the error terms $\varepsilon_i$ such that given a new $\mathbf x^*$ from a pair $(\mathbf x^*, y^*)$ with $y^*$ yet unknown, $f(\mathbf x^*)$ is a good advance prediction of $y^*$. The result is provided in the form of a set of possible functions $\{f_k\}$, chosen in such a way that the spread of $f_k(\mathbf x^*)$ over the possibilities quantifies the uncertainty in predicting $y^*$. See \citet[ch.~4]{muller2015}.

    \paragraph{BART in brief}

    BART represents the regression function as the sum of many decision trees. It puts a prior probability distribution over the space of all possible trees, and then samples from the distribution conditioned on the observed datapoints with a MCMC. This provides a posterior distribution over the space of trees, in the form of a set of random samples from such distribution, which represent the inference about which functions could describe the data. The pointwise average of these samples gives a single pointwise good estimate, while the pointwise standard deviation a pointwise uncertainty quantification.

    \paragraph{Implementations}

    The most popular implementations of BART are the R software libraries \texttt{bartMachine} \citep{kapelner2016,kapelner2023}, \texttt{dbarts} \citep{dorie2024}, and \texttt{BART} \citep{sparapani2021,mcculloch2024}, though many others exist.

    \subsection{Definition of the BART model and prior distribution}
    \label{sec:bartprior}

    I summarize the description from the original BART paper \citep{chipman2010}.

    \paragraph{Decision trees}
    
    A decision tree is a way to represent a stepwise function $\mathbb R^p \to \mathbb R$ using a binary tree. Each non-leaf node contains a \emph{decision rule}, consisting in a pair $(i, s)$ combining a splitting axis $i$ with a splitting point $s$ along such axis, partitioning the input space in two half-spaces along the plane $\{\mathbf x \in \mathbb R^p: x_i = s\}$, orthogonal to axis $i$. Each half is assigned to a children of the node. Children nodes in turn further divide their space amongst their own children. Leaf nodes contain the constant function value to be associated to their whole assigned rectangular subregion. A minimal tree consisting of a single root leaf node represents a flat function. See \autoref{fig:tree} for an illustration.

    \begin{figure}
        \includegraphics[width=\columnwidth]{fig1hill2020.pdf}
        \caption{\label{fig:tree} Depiction and terminology of a decision tree. Adapted from \citet{hill2020}.}
    \end{figure}

    \paragraph{Decision trees for regression}
    
    A decision tree can be used to represent a parametrized family of functions for regression. The parameters are the structure of the tree $T$, including splitting decisions, and the leaf values $M$. I indicate the function represented by the tree with $g(\mathbf x;T,M)$. To use this model in Bayesian inference, it is necessary to specify a prior distribution over $(T,M)$. I factorize the prior as $p(T,M) = p(M\mid T) p(T)$ and specify the two parts separately.

    \paragraph{Prior over tree structure}
    
    The distribution $p(T)$ is defined by the following recursive generative model \citep[see][]{chipman1998}. Fix an axes-aligned grid in the input space, not necessarily evenly spaced, by specifying a set of splitting points $S_i = \{s_{i1}, \ldots, s_{in_i}\}$ for each axis $i \in \{1,\ldots,p\}$. Start generating the tree from the root, descending as nodes are added. At each new node, consider the subset of splitting points which are available in its assigned subregion along each axis, and keep only the axes with at least one split available. If there are none, the node is a leaf. If there are any, decide with probability $P_d = \alpha/(1 + d)^\beta$ if the node is nonterminal, i.e., if it should have children, where $d$ is the depth of the node, $d=0$ at the root, and $\alpha \in [0, 1]$ and $\beta \in [0,\infty)$ are two fixed hyperparameters. If the node is nonterminal, its splitting axis is drawn uniformly at random from the allowed ones, and the splitting point along the chosen axis is drawn uniformly at random from the available ones.

    \paragraph{Prior over leaf values}
    
    If a node terminates, its leaf function value is sampled from a Normal distribution with mean $\mu_\mu$ and standard deviation $\sigma_\mu$, independently of other leaves. This defines $p(M\mid T)$.

    \paragraph{Complete regression model}
    
    The BART regression model uses a sum of $m$ independent trees, each with the prior above, plus the usual error term:
    %
    \begin{align}
        %
        &
        y_i = f(\mathbf x_i) + \varepsilon_i
        , \qquad
        f(\mathbf x) = \sum_{j=1}^m g(\mathbf x; T_j, M_j)
        , \label{eq:bartdef} \\
        %
        %
        &
        \varepsilon_i \mid\sigma \overset{\mathrm{i.i.d.}}\sim \mathcal \mathcal N(0, \sigma^2)
        , \qquad
        \frac{\lambda\nu}{\sigma^2} \sim \chi^2_\nu
        , \\
        %
        %
        &
        p(\{T_j\}, \{M_j\}) = \prod_{j=1}^m p(M_j \mid T_j) p(T_j).
        \label{eq:bartdef2}
        %
    \end{align}

    \paragraph{Hyperparameters}
    
    The free hyperparameters in the model, to be chosen by the user, are:
    %
    \begin{itemize}

        \item The splitting grid. The two common defaults are 100 equally spaced splits along each $\mathbf x$ axis in the range of the data, or a split midway between each observed value along each $\mathbf x$ axis.
        
        \item The number of trees $m$, default 200. Although ``$m$'' is the original notation, in the following I'll refer to this as ``$n_\text{tree}$'' for clarity.
        
        \item $\alpha$ and $\beta$, regulating the depth distribution of the
        trees. Default 0.95 and 2.
        
        \item $\mu_\mu$ and $\sigma_\mu$, which set the prior mean and variance
        $m\mu_\mu$ and $m\sigma_\mu^2$ of $f(\mathbf x)$. By default set based on some measure of the location and scale of the data.
        
        \item $\nu$ and $\lambda$, which regulate the prior on the variance of the error term. By default $\nu=3$, while $\lambda$ is set to some measure of squared scale of the data.

    \end{itemize}

    \subsection{BART inference procedure and posterior distribution}

    In this section I give an overview of the algorithm used in BART to ``learn'' the trees from the data. In the explanation I assume knowledge of the basic techniques used in Bayesian inference with MCMC; see \autoref{sec:bayesrecap} for a recap of the topic. For more complete details of the BART algorithm, see \citet{chipman1998,chipman2010}, \citet[\S A]{kapelner2016}, \citet[ch.~5]{daniels2023}, and \citet[\S2.1.4, p.~5]{tan2019}.

    \paragraph{The BART MCMC}

    BART uses Metropolis sampling and exact sampling within a Gibbs sampler. The partition for Gibbs is: the structure of each tree $T_j$, the leaves of each tree $M_j$, and the error variance $\sigma$. Each $T_j$ is sampled with Metropolis, while each $M_j$ and $\sigma$ are sampled exactly because their conditional posteriors are standard distributions. More in detail, each iteration of the MCMC proceeds as follows. A random small modification is proposed for $T_1$, proposing to grow a leaf into a node with two leaves, to prune two sibling leaves making their parent a leaf, or other more complex moves. It is either accepted or rejected. Then, $M_1$ is sampled given the potentially modified $T_1$; this step is a linear regression. This is repeated for all trees in order. Then $\sigma$ is sampled with the trees fixed; its conditional posterior is an inverse gamma distribution.

    \paragraph{Behavior of the BART MCMC}

    The following is based on my general experience with BART and with developing the implementation described in this paper. The acceptance of the tree structure Metropolis step is typically \SI{10}\%--\SI{30}\% \citep[see also][p.~891]{pratola2016}, so on average the whole forest moves every 3 to 10 MCMC iterations. Pathological cases can lower this acceptance to very small values. A rule of thumb on the number of samples is to run the chain for 2000 samples and discard the first 1000 (differently from the defaults of BART packages, which range from 100 to 500).
    %
    % default burn-ins on 2024-10-30:
    % 100 in BART, dbarts::bart, BayesTree
    % 250 in bartMachine
    % 500 in dbarts::bart2
    %
    See \citet[fig.~7, p.~20]{tan2024} for an analysis of the benefit of running multiple chains. It is useful to run at least 2 chains and compare the distributions of the function value at a few points; if the distributions do not coincide, it may be useful to burn more initial samples. Each tree should not end up with more than 10 leaves.

    \paragraph{Convergence of the BART MCMC}

    This MCMC is quite complex, and moves the tree structures slowly. In practice in most problems it will not reach convergence, i.e., it won't sample the tree structures from their actual posterior. However, the function values also depend on the leaves, which are re-sampled each time from scratch instead of updated with Metropolis, so the quality of the MCMC samples is sufficient for prediction, even if they are not representative of the precise tree distribution. See \citet[p.~12]{ronen2022} and \citet[p.~4]{tan2024}.

    \section{Implementation of BART on GPU}
    \label{sec:impl}

    This is the principal contribution of this paper. The implementation described here corresponds to version
    % X.X.X
    0.4.0
    of the Python package \texttt{bartz}
    \citep{petrillo2024b}%
    .

    \paragraph{Tooling}

    I write the code in Python using the machine learning framework JAX \citep{bradbury2018}. This stands in contrast to almost all of the existing implementations, that provide an R interface to lower-level code in C++ or Java. I believe the success of this implementation crucially depends on this choice of using modern ML tools instead of the ones typical of this field. JAX provides a computing toolset built around array objects, mimicking Numpy \citep{harris2020}, and optimizes and compiles the sequence of operations for CPU, GPU or TPU.

    \subsection{Parallelism and branchlessness}
    \label{sec:branchless}

    \paragraph{Parallel computation}

    JAX automatically converts all the operations written in Python to GPU code. However, to actually gain from using the GPU, the operations must be easily parallelizable. For example: adding two large arrays elementwise is ok, because each sum can be executed independently; while a sequence of many different operations on a few values has to be executed sequentially. The GPU is slow at doing things in sequence, but can do many things at once.

    \paragraph{Branchless computation}

    Another algorithmic property important for exploiting JAX and the GPU effectively is the absence of branching. Branching means changing the sequence of planned operations based on a contextual condition; for example, an \texttt{if} clause that depends on the output of the previous calculation is branching, while an \texttt{if} clause that depends on a global configuration constant is not branching. In branchless code, the full sequence of arithmetic operations done by the code can be determined in advance, allowing to plan and optimize for optimal use of the GPU.

    \paragraph{Branchless \& parallel BART}

    The BART MCMC algorithm as commonly implemented does not have these properties: the sizes of the trees change as the algorithm runs, the size-changing modifications to the trees are accepted or rejected randomly, and each tree is processed after the previous one, so it is not a pre-determined sequence of arithmetic operations across arrays. However, with a slight variation, it becomes possible to re-express the algorithm in such a way. The variation is fixing a maximum depth for the trees. The BART prior is designed to favor shallow and balanced trees, and the well-functioning of the MCMC relies on the trees being shallow, so this is not a problematic restriction.

    \paragraph{Bottom line}

    To write an efficient JAX implementation of BART, I have to express the BART MCMC procedure as pre-determined arithmetic operations on large matrices.

    \subsection{Representation of variables}
    \label{sec:repr}

    \paragraph{Trees}

    My goal is representing the trees in computer memory with a few large matrices of fixed size. The simplest way of doing this is with a heap, illustrated in \autoref{fig:heaptree}.
    %
    \begin{figure}
        \halfcenter{\includegraphics[width=0.75\columnwidth]{heaptree}}
        \caption{\label{fig:heaptree} Heap representation of a decision tree, with the maximum depth set to 3.}
    \end{figure}
    %
    Each tree is represented by 3 arrays, separately holding the axis and splitting point for decision nodes, and the value for leaf nodes. The arrays are fixed in size, and each index corresponds to a potential node in the tree. The indices are assigned to the (potential) tree nodes in order, starting from 1, top to bottom, left to right (like reading text), imagining a full, balanced binary tree with the maximum allowed depth. The entries corresponding to nodes not actually present in the tree are ignored, as are the axis and cutpoint entries (resp.\ the leaf value entry) corresponding to leaf (resp.\ decision) nodes. I fix the convention that the cutpoints start from 1, and that a 0 value in the cutpoint array entry marks the node as a leaf. The set of trees is then represented by 3 matrices where each row is a single tree array. I set as default, and always use in this article, maximum depth $D=6$, where $D=1$ for a root-only tree. See \autoref{sec:impldetails} for an analysis of the size of this representation.

    \paragraph{Predictors}

    Each predictor $x_i$ can be replaced by its index into the splitting grid. Since the number of cutpoints rarely exceeds 255 in practice (the default in BART packages is 100), each predictor then requires only 1 byte, for a total of $np$ bytes for the matrix of predictors. This size reduction is important because the whole matrix has to fit into the GPU memory, and it probably also speeds up fetching the predictors from memory as the algorithm runs.

    \subsection{The algorithm}

    I explain in full detail how to traverse the decision trees, as a simple illustration of how to write vectorized code, then describe the rest of the algorithm only in broad terms. See the software package documentation for details; the internals are documented.

    \paragraph{Traversing the trees}

    By ``traversing'' a decision tree I mean finding the leaf associated with a given $\mathbf x$ vector. To fix in advance the sequence of operations irrespectively of the actual position in the tree of the leaf, this procedure can be implemented as a loop through the levels of the tree, up to the maximum possible depth, carrying a flag to indicate whether the leaf has already been reached or not. This way the number of iterations of the loop does not depend on the depth of the target leaf. See \autoref{alg:traverse}.
    %
    \begin{algorithm}[t]
        \caption{\label{alg:traverse} Branchless traverse of a decision tree}
        \begin{algorithmic}
            \STATE \textbf{Input:} Predictors $\mathbf x$, and the rules of a decision tree represented by axis vector $\mathbf a$ and cutpoint vector $\mathbf s$
            \STATE \textbf{Output:} Heap index of the leaf $\mathbf x$ falls into
            \STATE leaf\_found = False
            \STATE index = 1
            \FOR{i = 1 \textbf{to} $D - 1$}
                \STATE axis = $a_\text{index}$
                \STATE split = $s_\text{index}$
                \STATE leaf\_found = leaf\_found OR (split = 0)
                \STATE child\_index = $2 \times \text{index} + (\text{1 \emph{if} $x_\text{axis} \ge \text{split}$ \emph{else} 0})$
                \STATE index = index \emph{if} leaf\_found \emph{else} child\_index 
            \ENDFOR
            \STATE \textbf{return} index
        \end{algorithmic}
    \end{algorithm}
    %
    JAX automatically vectorizes the code: each operation in the traverse is repeated for all points and all trees. To repeat, it is not the whole algorithm which is run once for each combination of $\mathbf x$ and tree, but rather each operation in the algorithm is run on all combinations before moving to the next instruction, and the scalar variables become vectors or matrices as needed.

    \paragraph{Tree structure proposal}

    I have to propose a modification to each tree structure as moves for the Metropolis steps. The original BART picks at random amongst 4 types of moves: GROW (add two children to a leaf node), PRUNE (remove two sibling leaves), SWAP (permute the parameters of two directly connected decision nodes), and CHANGE (change the parameters of a decision node) \citep[\S5.1, p.~940]{chipman1998}. Following the implementation of \texttt{BART}, I use only GROW and PRUNE for simplicity \citep[\S C, p.~57]{sparapani2021}. To stay branchless, I have to execute all the calculations for both moves, even if I end up using only one. GROW and PRUNE have the nice properties that the modifications can not overlap in the tree arrays, and that the tree proposed by GROW contains the initial tree which in turn contains the tree proposed by PRUNE. This allows to share the memory and most of the calculations between the two moves, and simplifies the operations with indices and residuals described next.

    \paragraph{Traverse indices cache}

    The indices that, for each tree, indicate in which leaf each datapoint falls into, are used to compute the acceptance probability of the Metropolis steps, and to sample the leaves. At each MCMC iteration, these indices change in limited ways due to the simplicity of the GROW and PRUNE moves, so it is convenient to keep the indices in memory rather than recompute them from scratch each time with \autoref{alg:traverse}. If the maximum depth of the trees $D$ is $\le 8$, the indices fit into a byte, so this cache uses $n \times n_\text{tree}$ bytes. Together with the $n\times p$ predictors matrix, this is the bulk of the memory usage of this algorithm.

    \paragraph{Residuals cache}

    The residuals, i.e., the difference between the data $\mathbf y$ and the predictions of the tree ensemble, appear in many calculations in the MCMC. Computing the residuals is somewhat expensive, but they change progressively as each tree is modified in turn, so I keep a cache of residuals and, after modifying each tree, I update them by adding the difference between the old and new leaves of the tree (properly broadcasted to all the datapoints).

    \paragraph{Computations parallel across trees}

    The parts of the algorithm that can be parallelized across trees are:
    %
    \begin{enumerate}
        \item Propose the moves: since which move to use and the nodes to grow/prune are picked at random, there is no sequential dependence.
        
        \item Update the traverse indices cache based on the largest tree considered in each move: proposed for GROW, initial for PRUNE. The indices for the smaller version of the tree can be computed from these; this is done later on if the move is rejected/accepted.
        
        \item Count the number of points falling in each leaf. This is used to evaluate the acceptance probability and to sample the leaves. This only depends on the proposed moves.
        
        \item Compute the variance of the conditional posterior of each leaf. Since the calculation is a linear regression with Normal distributions, the variance only depends on the number of points and not on the values of the leaves, so there is no sequential dependence.
        
        \item Generate random samples. All random samples can be generated as standard uniform or Normal and transformed later to the target distribution. The leaves are  scaled immediately to their variance but left centered.
        
        \item Compute most of the terms in the Metropolis acceptance probabilities; similarly to the variances, many depend only on the number of points and fixed parameters.
    \end{enumerate}

    \paragraph{Computations sequential along trees}

    The parts that can't be parallelized are:
    %
    \begin{enumerate}[start=7]
        \item Sum the residuals falling in each leaf (bottleneck of the algorithm).

        \item For each leaf, add the initial leaf value times the number of points to the sum of residuals, to remove the effect of the current tree.

        \item Complete the calculation of the Metropolis acceptance probability and accept/reject the move.

        \item Compute the posterior mean of the leaves and add it to the pre-computed centered leaves.

        \item Add to the residuals the difference between the initial and new predictions of the tree.
    \end{enumerate}

    \paragraph{Bottleneck: summing residuals}

    The part that can not be parallelized across trees is problematic for the GPU. The main operation is summing the residuals. This kind of operation is called ``indexed reduce'': the residuals are summed in different accumulators (one per leaf), and which accumulator to use for each value is indicated by an array of indices (the traverse indices). This takes $O(n)$. Thus the GPU is fully utilized only if $n$ is high enough. As shown in \autoref{sec:speed} below, this can be a problem in practice.

    \paragraph{Sampling the error variance}

    After the tree steps, the error variance is sampled using the sum of squared residuals to compute its conditional posterior. With many datapoints, the distribution is narrow, so this is approximately equivalent to setting the error variance to the variance not explained by the current trees.

    \subsection{Software package}

    I implemented the algorithm in the Python package \texttt{bartz}, distributed through PyPI
    \citep{petrillo2024b}%
    . The code is available under the permissive MIT open-source license. The correctness of the implementation is checked with automatic unit tests. To ease adoption for BART users, I provide an interface (in addition to lower-level functions) that imitates \texttt{BART}, which in turn shares most of the interface with \texttt{dbarts} and the historical implementation \texttt{BayesTree}. Some unit tests check that the samples produced by \texttt{bartz} and \texttt{BART} with the same configuration represent the same posterior. The prototype usage is
    %
    \lstset{
        language=Python,
        basicstyle=\ttfamily\footnotesize,
        commentstyle=\itshape,
    }
\begin{lstlisting}
import bartz
X, y, X_test = ... # fetch data
bart = bartz.BART.gbart(X, y, ...) # run
y_pred = bart.predict(X_test) # predict
\end{lstlisting}

    \section{Performance measurements}
    \label{sec:perf}

    In \autoref{sec:speed} I measure the running speed and memory usage of the algorithm, while in \autoref{sec:pred} I check its correctness.

    \subsection{Speed benchmark}
    \label{sec:speed}

    \paragraph{General setup}

    I pit my implementation \texttt{bartz} on CPU and GPU against \texttt{dbarts} (the fastest BART implementation) and \texttt{XGBoost}.\marginpar{Add XBART} My CPU is a single core of an Apple M1 Pro. As GPU, I use the Nvidia L4 for \texttt{XGBoost} and the Nvidia A100 for \texttt{bartz}. The L4 is smaller than the A100, but that does not make a difference for \texttt{XGBoost}, and it was more convenient to access. As performance metric I use the time per iteration (see \autoref{sec:itertime} in the appendix for details); for BART, this is the time of a MCMC iteration, while for \texttt{XGBoost} this is the time to build all the trees, so they are not directly comparable. BART requires $O(1000)$ iterations for the MCMC, while \texttt{XGBoost} $O(100)$ for a potential cross validation, but can also be run just once.

    \paragraph{Data generating process}

    I run the benchmark on synthetic data for convenience in setting arbitrarily the training set size $n$ and number of predictors $p$. For \texttt{bartz} I use a not particularly meaningful data generating process (DGP), designed purely for convenience in avoiding out-of-memory errors when generating large datasets:
    %
    \begin{align}
        y_i &= \cos\left( \frac{2n\pi}{32} \frac{i-1}{n-1} \right), \quad i = 1,\ldots, n, \\
        X_{ij} &= (i + (p+1)j) \bmod 256.
    \end{align}
    %
    Since \texttt{bartz} is branchless, the running time does not depend on the DGP, so the DGP does not need to be realistic. Instead, for \texttt{dbarts} and \texttt{XGBoost}, I have to specify a DGP that makes sense, otherwise their running time may vary substantially. I use
    %
    \begin{align}
        y_i &= \frac1{\sqrt p}\sum_{j=1}^p \cos(\pi X_{ij}) + \varepsilon_i, \\
        X_{ij} &\overset{\mathrm{i.i.d.}}{\sim} U(-2, 2), \\
        \varepsilon_i &\overset{\mathrm{i.i.d.}}{\sim} \mathcal N(0, 0.1^2).
    \end{align}
    %
    This is designed to be a realistic but ``easy'' DGP. The total data variance is $\operatorname{Var}[\mathbf y]\approx 1$. $X$ is uniformly distributed, such that the evenly spaced splitting grid used by default in \texttt{dbarts} is optimal. The error standard deviation is 0.1, so the data is low noise enough for \texttt{XGBoost} to work well, but not so low noise that BART would have low acceptance problems in the MCMC. The model does not have interactions, so the algorithms should always find good decision rules.

    \paragraph{Size settings}

    The number of predictors and the number of trees influence the running time and the memory usage, so I repeat the benchmark on a $2\times 2$ grid of settings: low/high p $\times$ low/high number of trees. As ``low'' settings I use $p=100$, $n_\text{tree} = 200$, while as ``high'' I make the parameters grow proportionally to $n$ with $n/p = 10$ and $n/n_\text{tree}=8$. The number of trees being proportional to $n$ is an unusually high setting.\marginpar{This number of trees probably is too crazy high for XGBoost. I don't know how to set it sensibly. It shouldn't stay constant either.} These combinations are not optimal for inference, they are simple extreme cases to understand the computational complexity of the algorithms. The ``low-low'' setting is the one closer to common usage.

    \paragraph{Results}

    The results with the ``low-low'' settings $p=100$, $n_\text{tree}=200$ are shown in \autoref{fig:time-single} (complete results in \autoref{fig:time-all} in the appendix). \texttt{dbarts-cpu} is 3-4 times faster than \texttt{bartz-cpu} at low training set size $n$, then \texttt{bartz-cpu} catches up at $n\approx \num{20000}$ and they remain similar afterward. \texttt{bartz-A100} takes a constant time until $n \approx \num{1000000}$ where it starts following a linear trend, indicating full use of the GPU. At high $n$, \texttt{bartz-A100} is 200x faster than \texttt{bartz-cpu}, and 1000x faster than \texttt{xgboost-L4}, making BART competitive with \texttt{XGBoost} after taking into account the different number of iterations required.

    \begin{figure}
        \halfcenter{\includempl{time-single}}
        \caption{\label{fig:time-single} Time to run an iteration of the algorithm vs.\ training set size $n$, comparing \texttt{bartz} with the fastest BART implementation (\texttt{dbarts}) and \texttt{XGBoost}, on CPU (\texttt{-cpu}) and GPU (\texttt{-A100}, \texttt{-L4}). Keep into account that \texttt{XGBoost} requires only one iteration to produce a usable result, while BART requires $O(1000)$ iterations.}
    \end{figure}

    \paragraph{Memory usage}

    I did not measure memory usage programmatically, but I observed on the system monitor that \texttt{bartz} uses $\approx n(p+n_\text{tree})$ bytes as expected, while \texttt{dbarts} uses $\approx 24 n(p+n_\text{tree})$. In my personal experience, memory usage is the showstopper in analysing large datasets.\marginpar{someone in the literature said the same, but I can't remember who} \texttt{dbarts} is the fastest package, but not the one with the lowest memory usage, as it caches the tree traverse like \texttt{bartz}, so this comparison does not necessarily make \texttt{bartz} state of the art re memory. The memory requirements would increase if the number of cutpoints was $>255$ or the maximum tree depth $>8$.

    \paragraph{Caveats about practical performance}

    Since these benchmarks measure only the time taken by MCMC iterations, they are evaluating stripped-down versions of the full BART method, both for \texttt{bartz} and \texttt{dbarts}. In particular, if \texttt{bartz} was run through its \texttt{BART}-like user interface instead of the lower-level functions, passing the predictors matrix in as, say, 32 bit floating points, it would run out of GPU memory earlier as it is likely the case that $4np > n(p+n_\text{tree})$. Squeezing the most out of the GPU requires pre-processing the data on CPU and moving to GPU the MCMC-ready matrices.

    \subsection{Predictive accuracy}
    \label{sec:pred}

    Since I re-implemented a well known algorithm, with unit tests checking that it yields the same results as an existing implementation, there is no need to measure its statistical performance thoroughly. Anyway, to show at least one concrete piece of evidence that everything is working correctly, I run a comparison of the out-of-sample prediction error\marginpar{Measure also coverage.} between my package \texttt{bartz} and the 3 most popular BART implementations, on data simulated from a simple model with a dense linear term and a sparse quadratic term. I use the same ``low/high'' settings of \autoref{sec:speed} for $p$ and $n_\text{tree}$. The result at ``low-low'' settings is in \autoref{fig:rmse-single} (complete results in \autoref{fig:rmse-all} in the appendix); the RMSEs shake out similar to each other as expected.

    \begin{figure}
        \halfcenter{\includempl{rmse-single}}
        \caption{\label{fig:rmse-single} Test set root mean square error (RMSE) vs.\ training set size, comparing \texttt{bartz} with the most popular implementations of BART, to check they produce the same results. The test set size is always 1000.}
    \end{figure}

    \paragraph{Data generating process}

    The data is simulated from the model
    %
    \begin{align}
        y_i &= \frac 1{\text{norm.}} \sum_{j=1}^p X_{ij} \beta_j + {} \notag \\
        %
        &{} + \frac 1{\text{norm.}} \sum_{j=1}^p \sum_{k=1}^p A_{jk} X_{ij} X_{ik} + \varepsilon_i, \label{eq:rmsedgp} \\
        \varepsilon_i, \beta_j &\overset{\text{i.i.d.}}{\sim} \mathcal N(0, 1), \\
        X_{ij} &\overset{\text{i.i.d.}}{\sim} U(0, 1), \\
        A_{jk} &\begin{cases}
            \overset{\text{i.i.d.}}{\sim} \mathcal N(0, 1) & \text{if $\min(|j - k|, p - |j - k|) < 5$}, \\
            =0 & \text{otherwise},
        \end{cases}
    \end{align}
    %
    where the denominators ``norm.'' indicate standardization of the attached term to unit sample variance after generating the data, train and test set together. This step is improper, but the leakage it causes is small at high $n$, and the comparison is relative anyway. This standardization simplifies interpreting the RMSE, as the total variance of the data stays at about 3.

    \section{Conclusions}
    \label{sec:conclusions}

    \paragraph{Main result}

    I devised a branchless version of the BART MCMC algorithm to run BART on GPU. This achieves a speedup sufficient to make BART competitive with \texttt{XGBoost}, the most popular tree-based method, making BART a concrete alternative for big datasets. The maximum speedup is reached only with $n\gtrsim\num{1000000}$, but this is the regime that matters anyway, as speed is less of a problem with less data.

    \paragraph{Mild limitations}

    Compared to \texttt{XGBoost}, the first evident limitation of my software package \texttt{bartz} is that it does not implement regression with binary outcomes, which would be straightforward to add following the BART literature. The second one is that it runs on only one CPU core or GPU at once, while \texttt{XGBoost} allows to split data and processing across machines. This feature is straightforward to add as well, as JAX scales to large clusters, and the BART MCMC allows efficient sharding of the data \citep{pratola2014}; I simply have not put in the work. In general my software is not mature and so does not provide a truly viable competitor to \texttt{XGBoost} right away.

    \paragraph{Deeper limitations}

    My implementation runs out of GPU memory when the number of predictors $p$ or the number of trees are high (see \autoref{sec:gpumem} in the appendix). This problem is unavoidable. The implementation already uses only 1 byte per predictor and 1 byte per tree index, so it's close to the minimum memory requirements. The indices cache could potentially be dropped, slowing down the algorithm, or batched from CPU memory; but the predictors matrix is always used all at once. Even though it is possible to adapt the algorithm to run on multiple GPUs, the partition across GPUs has to be along the observations, not the predictors: if each GPU stores a small number of observations to fit a large number of predictors, there is no speed advantage.

    \paragraph{Future directions}

    Other than mundane improvements to the software, the most useful addition would be to re-implement XBART \citep{he2021} for GPU as well. If XBART runs overall 20-30 times faster than BART, and \texttt{bartz} is 200x faster on GPU than the state of the art on CPU, hopefully XBART on GPU could be 4000x faster, making it decidedly advantageous compared to \texttt{XGBoost} or any other method. Since XBART uses an expensive single tree sampling process, scanning many predictors and splitting points to find good decision rules, it could potentially parallelize better at lower $n$, allowing to shard high-$p$ problems across GPUs without losing efficiency.

    \section*{Acknowledgements}

    % \textless REDACTED FOR ANONYMITY\textgreater

    I thank Antonio Linero, Andrew Herren, Richard P.\ Hahn, and Francesco C.\ Stingo for useful comments and suggestions. I did this work as a PhD candidate at the Department of Statistics, Computer Science, Applications (DISIA) ``G.~Parenti'' of the University of Florence (UNIFI), Italy, and as a visiting researcher at the Department of Statistics and Data Science (SDS) of the University of Texas at Austin (UT Austin), USA. The introductory material in sections~\ref{sec:intro} and~\ref{sec:bartprior} is adapted from \citet{petrillo2024f} by the same author.

    \section*{Code and data}

    The results in this paper are fully reproducible with
    % \textless REDACTED FOR ANONYMITY\textgreater
    \citet{petrillo2024e}.

    % SEE SUPPLEMENTARY MATERIAL

    \section*{Statement of impact}

    % verbatim snippet as per ICML guidelines
    This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of my work, none which I feel must be specifically highlighted here.

\bibliography{bibliography}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix

    \section{Recap on Bayesian inference with MCMC}
    \label{sec:bayesrecap}

    \paragraph{Bayesian inference}

    In the Bayesian paradigm, the parameters of the model (and consequently the predictions) are inferred from the data by conditioning the prior probability distribution on the observed values of the data, obtaining a posterior distribution for the parameters. The BART model definition in Equations~\ref{eq:bartdef}--\ref{eq:bartdef2} can be expanded into a joint probability density function of the data and parameters $p(\mathbf y, \sigma, \{T_j\}, \{M_j\})$; the posterior $p(\sigma, \{T_j\}, \{M_j\}\mid \mathbf y)$ is, up to a constant multiplicative factor, the same expression but with $\mathbf y$ held fixed at the observed values. This distribution expresses the degree of belief in each possible combination of parameter values being the correct one; e.g., the mode or the mean of the distribution could be used as estimates of the parameters.

    \paragraph{MCMC}

    Even though the posterior $p(\sigma, \{T_j\}, \{M_j\}\mid \mathbf y)$ could be written down explicitly, it is a long and complex formula that is in practice impossible to interpret. To represent the posterior in a legible way, the most common technique is Markov chain Monte Carlo (MCMC): setting $\theta=(\sigma, \{T_j\}, \{M_j\})$ for brevity, it is possible to define an iterative numerical procedure that, starting from an initial choice of parameter values $\theta^{(0)}$, evaluating numerically the posterior density function $\theta\mapsto p(\theta|\mathbf y)$ only in some points, picks a new set of values $\theta^{(1)}$, and so on $\theta^{(2)}, \theta^{(3)}, \ldots$, yielding overall a sequence of parameter values which is a random sample from the posterior. Then, e.g., the mean of the distribution is approximated by the average of these samples. See \citet{brooks2011} for a reference on MCMC.

    \paragraph{Properties of MCMC}

    The samples produced by MCMC are only asymptotically correct: the chain takes some time to ``forget'' the arbitrary initial state $\theta^{(0)}$ and start sampling from the target distribution $p(\theta|\mathbf y)$. They are also not independent, as each sample is picked starting from the previous one. In practice, some of the initial samples must be discarded, and even after that, the remaining number of MCMC samples must be higher by some factor than what's sought.

    \paragraph{Metropolis sampler}

    A common general way of building a MCMC is the Metropolis-Hastings (MH) class of samplers. A MH sampler requires to define a \emph{proposal distribution} $p(\theta';\theta)$. At MCMC step $k$, the current values of the parameters $\theta^{(k)}$ in the chain are plugged as parameters of the proposal as $p(\cdot;\theta^{(k)})$. A random draw $\tilde\theta^{(k+1)}\sim p(\cdot;\theta^{(k)})$ from the proposal gives potential new parameter values $\tilde\theta^{(k+1)}$. Then a criterion decides if the new proposed values are accepted (keep new values, $\theta^{(k+1)} = \tilde\theta^{(k+1)}$) or rejected (keep current values, $\theta^{(k+1)} = \theta^{(k)}$), with a biased coin flip that depends on the posterior and favors the choice of $\theta^{(k+1)}$ that makes the posterior density higher. In other words, the sampler tries to move one step at a time in parameter space, picking a move at random, testing the ground, and deciding whether to proceed in that direction, based on how much the probability increases going there.

    \paragraph{Gibbs sampler}

    Another general way of defining a MCMC is Gibbs sampling, based on partioning the procedure across parameters. If there are multiple parameters (e.g., $\theta \in \mathbb R^d$ with $d > 1$), it is difficult to devise a good proposal distribution for Metropolis, or to find other valid schemes in general. It is possible to sample only one parameter $\theta_i$ (or one subset of parameters) at a time, while keeping the other parameters fixed to their current values, as if those other values were known in advance. Iterating this over all parameters produces a valid MCMC.


    \section{Further details on the implementation}
    \label{sec:impldetails}

    \paragraph{Branchless \& parallel on CPU}

    With reference to \autoref{sec:branchless}: modern CPUs benefit as well, to a lower degree than GPUs, from branchless and parallelizable code. Branchlessness makes RAM access patterns predictable, which speeds up fetching data from memory (the slowest operation for the CPU), while parallelization happens at the levels of vectorized instructions and multiple cores. This means that, even if the implementation is designed for GPU, it could be performant on CPU as well; this is confirmed by the benchmark in \autoref{sec:speed}.

    \paragraph{Size of the tree representation}

    With reference to \autoref{sec:repr}: the heap representation of the trees wastes most of the array entries. However it turns out to use about the same memory as other common alternatives. The maximum number of nodes is $2^D - 1$, so the heap size is $2^D$ (index 0 is unused). Since the nodes at maximum depth can only be leaves, the axis and cutpoint arrays are only $2^{D-1}$ long. Using 32 bit integers and floats, the total size in bytes is $4 \times (2^{D-1} + 2^{D-1} + 2^{D}) = 2^{D+3}$ bytes. If $D=6$, that's 512 bytes. Consider instead a representation in C/C++ that used separately allocated linked node objects. Each node contains allocation metadata (16 bytes), axis (4), cutpoint (4), leaf value (4), and pointers to children ($2\times 8$), for a total of 44 bytes per node, rounded to 48 for alignment. This matches the size of the heap if there are 10 nodes per tree on average, and it's also not contiguous in memory. Other possible linked schemes, and those used in existing implementations, similarly consume a lot of memory in scaffolding; for example, \texttt{BART} uses 64 bytes per node. % see BART 2.9.6 files src/tree.h:100, src/tree.cpp:56

    \paragraph{Numerical error due to caching residuals}

    Since the residuals are cached and updated according to tree modifications, instead of recomputed, a potential problem is the accumulation of numerical error. The values of the residuals and their updates are similar across datapoints and trees, so, with 32 bit floats, each MCMC iteration adds a relative numerical error with standard deviation $\approx \num{1e-7} \sqrt {n_\text{tree}}$. This is not a problem for all current BART applications, but it would be with 16 bit floats, which might be an option to further accelerate the implementation.


    \section{Complete results of performance measurements, and further details}

    The speed benchmarks and out-of-sample RMSE tests for all combinations of $p$-$n_\text{tree}$ settings are in Figures~\ref{fig:time-all} and~\ref{fig:rmse-all}.

    \subsection{Precise definition of iteration time}
    \label{sec:itertime}

    For BART, I am careful to measure only the time to run an iteration, after warming up, and not the time taken by initialization or post-processing. This is possible because both my package and \texttt{dbarts} allow to run each step of the algorithm separately. For \texttt{XGBoost}, instead, I measure the time to run it once with its default configuration; I expect this is indeed the time each fold in a CV would take and so it is the correct measure to make comparisons.\marginpar{I would really like an XGBoost expert to weigh in.}

    \begin{figure*}
        \widecenter{\includempl{time-all}}
        \caption{\label{fig:time-all} Time to run an iteration of the algorithm vs.\ training set size $n$, comparing \texttt{bartz} with the fastest BART implementation (\texttt{dbarts}) and \texttt{XGBoost}, for various settings of number of trees and number of predictors $p$, on CPU (\texttt{-cpu}) and GPU (\texttt{-A100}, \texttt{-L4}). Keep into account that \texttt{XGBoost} requires only one iteration to produce a usable result, while BART requires $O(1000)$ iterations.}
    \end{figure*}

    \subsection{Effect of limited GPU memory}
    \label{sec:gpumem}

    A striking feature of the speed benchmarks in \autoref{fig:time-all} is that with high $p$ or $n_\text{tree}$ the \texttt{bartz-A100} line terminates at lower $n$, despite not taking much time. This is because I run out of GPU memory. The memory of GPUs is fixed, it can't be expanded, and there is no virtual memory, so this is an unavoidable limit. The speed premium of the GPU kicks in only at high $n$, so the memory limit indirectly limits the speed gain. The practical consequence is that running the algorithm in ``high'' $p$ or $n_\text{tree}$ settings allows a speed gain of at most 20--40x rather than the maximum 200x.

    \begin{figure*}
        \widecenter{\includempl{rmse-all}}
        \caption{\label{fig:rmse-all} Test set root mean square error (RMSE) vs.\ training set size, comparing \texttt{bartz} with the most popular implementations of BART, to check they produce the same results, for various settings of number of trees and number of predictors $p$. The test set size is always 1000.}
    \end{figure*}

    \subsection{On increasing the number of trees}
    \label{sec:moretrees}

    In this section I discuss how the speed of this implementation allows to run BART with many more trees than usual, which is an unexplored route in the BART literature, and potentially connects to various scaling behaviors of BART.

    \paragraph{Effect of forest size on predictive performance}

    I look at my RMSE measurements to get some empirical evidence that increasing the number of trees is useful. Consider the top panels of \autoref{fig:rmse-all}. They both have the ``high'' $p$ setting ($n = 10p$), while differring in the number of trees. In the right panel, at fixed $n_\text{tree}=200$, the RMSE at some point starts going up with $n$, while in the left panel it remains constant. This indicates that, at least with this kind of DGP, the number of trees should grow with $p$ (not necessarily linearly).
    %Similarly, although much less markedly, comparing the bottom panels suggests that, as $n$ increases, at some point the number of trees should increase as well.\marginpar{I wrote this because of the upward tail in low-low, right? But that may be because of the increasing total variance.}

    \paragraph{Typical forest size in the literature}

    Nobody, that I know of, ever explored the effect of increasing the number of trees this much. At ``high'' $n_\text{tree}$, the series stop at $n\approx 8000$, which means $n_\text{tree}\approx 1000$. I've never seen more than 200 trees in the literature, but for \citet[fig.~6, p.~286]{chipman2010} who reach $m=300$ as an experiment.

    \paragraph{Connection between forest size and MCMC behavior}

    The number of trees bears on the acceptance of the BART MCMC, together with the level of noise in the data. If the number of trees is too small, each tree has to grow deep to do its part in explaining the patterns in the data, and the MCMC gets stuck because ungrowing the trees is disfavored, and so they can't be changed in little steps; while with enough trees, each tree has to explain only a small part of the variation, so the prior prevails and keeps them shallow. The level of noise (i.e., unpredictable variation) has a similar effect \citep{pratola2016}: if the data is precisely predictable (for concreteness, picture $\operatorname{Std}[\mathbf y]/\sigma = 100$), the trees quickly grow to explain the variation, the error variance is shrunk accordingly, and once it is small, any small change that doesn't preserve a nigh-perfect data fit is too disfavored, so the MCMC gets stuck. Thus the signal/noise ratio (SNR) is an important influence on the optimal number of trees; high SNR datasets may need more trees.

    \paragraph{Semi-formal argument on optimal forest size}

    I now make an informal argument about what the number of trees should be. The number of leaves in each tree is not fixed, but at equilibrium and with many trees there will be a typical value. As the leaf values determine the values of the function, the total number of leaves in the ensemble is the number of parameters of the regression. This number should be equal to the total number of degrees of freedom required to explain the function behind the data on the observed region. An upper bound on this is the number of observed points $n$. If the number of leaves per tree has to be $\lesssim 10$ for the well-functioning of the MCMC, then in the worst case there should be at least $n/10$ trees. The effective number of parameters depends on the properties of the function; for example, under this view, the increasing RMSE($n$) (with $p \propto n$) in the top-right panel of \autoref{fig:rmse-all} is due to the true function (\autoref{eq:rmsedgp}) having a number of parameters $\propto p$.

    \paragraph{Potential implications}

    These considerations, together with the absence of high-$n_\text{tree}$ experiments in the literature, suggest this as an interesting area to explore, made accessible by this work as it alleviates the computational burden. I consider three specific examples:

    \begin{itemize}

        \item \citet[\S4.1]{ronen2022} show numerically how the convergence of the MCMC degrades at high $n$. Their experiment is already computationally demanding, and they stick to the default 200 trees, but a higher number of trees might improve the mixing.

        \item \citet{pratola2016} develops complex Metropolis proposals to increase acceptance in high-$n$, low-$\sigma$ datasets; a simpler alternative solution might be again to increase the number of trees.

        \item \citet{tan2024} study how the MCMC convergence deteriorates increasing $n$; they also provide some evidence on the usefulness of more trees in fig.~4, p.~16, but trying only up to 10 trees at $p=10$. Seemingly, they consider various more complex alternatives more interesting for improving mixing than simply trying more trees.

    \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
